[INFO 2018-01-29 17:32:10,814 main_a3c.py:116] Namespace(batch_size=64, discount_factor=0.99, ego_centric=False, gamma=0.99, gpu_id=1, image_block_size=64, init_model_path='./final.pth', keep_command=False, learn_start=0, learning_rate=0.0001, log_dir='./log', log_interval=10, map_config='../empty_ground.json', max_episode_length=100, method='actor_critic', num_games=10000, num_processes=4, opt_method='adam', pause_screen=False, render=False, replay_memory_size=100000, save_dir='./', save_interval=1000, seed=432, show_frame=False, single_word=False, teacher_type='NAVI_GOAL', tensorboard_path='../../tensorboard/exp001_01', test=False, train=True, use_gpu=False, view_map=False, visible_radius_unit=1)
[INFO 2018-01-29 17:32:10,815 xworld_utils.py:18] loading item list
[INFO 2018-01-29 17:32:10,815 xworld_utils.py:27] finish loading item list
[INFO 2018-01-29 17:32:10,815 xworld_utils.py:32] loading item images
[INFO 2018-01-29 17:32:10,888 xworld_utils.py:43] finish loading item images
[INFO 2018-01-29 17:32:10,889 xworld_map.py:29] loading ../empty_ground.json
[INFO 2018-01-29 17:32:10,889 xworld_map.py:45] finish loading ../empty_ground.json
[INFO 2018-01-29 17:32:12,646 main_a3c.py:140] training
[INFO 2018-01-29 17:32:16,113 xworld_utils.py:18] loading item list
[INFO 2018-01-29 17:32:16,114 xworld_utils.py:27] finish loading item list
[INFO 2018-01-29 17:32:16,114 xworld_utils.py:32] loading item images
[INFO 2018-01-29 17:32:16,203 xworld_utils.py:43] finish loading item images
[INFO 2018-01-29 17:32:16,204 xworld_map.py:29] loading ../empty_ground.json
[INFO 2018-01-29 17:32:16,204 xworld_map.py:45] finish loading ../empty_ground.json
[INFO 2018-01-29 17:32:16,216 xworld_utils.py:18] loading item list
[INFO 2018-01-29 17:32:16,216 xworld_utils.py:27] finish loading item list
[INFO 2018-01-29 17:32:16,216 xworld_utils.py:32] loading item images
[INFO 2018-01-29 17:32:16,281 xworld_utils.py:18] loading item list
[INFO 2018-01-29 17:32:16,281 xworld_utils.py:27] finish loading item list
[INFO 2018-01-29 17:32:16,282 xworld_utils.py:32] loading item images
[INFO 2018-01-29 17:32:16,320 xworld_utils.py:43] finish loading item images
[INFO 2018-01-29 17:32:16,321 xworld_map.py:29] loading ../empty_ground.json
[INFO 2018-01-29 17:32:16,321 xworld_map.py:45] finish loading ../empty_ground.json
[INFO 2018-01-29 17:32:16,386 xworld_utils.py:43] finish loading item images
[INFO 2018-01-29 17:32:16,386 xworld_map.py:29] loading ../empty_ground.json
[INFO 2018-01-29 17:32:16,386 xworld_map.py:45] finish loading ../empty_ground.json
[INFO 2018-01-29 17:32:16,485 xworld_utils.py:18] loading item list
[INFO 2018-01-29 17:32:16,486 xworld_utils.py:27] finish loading item list
[INFO 2018-01-29 17:32:16,486 xworld_utils.py:32] loading item images
[INFO 2018-01-29 17:32:16,591 xworld_utils.py:43] finish loading item images
[INFO 2018-01-29 17:32:16,591 xworld_map.py:29] loading ../empty_ground.json
[INFO 2018-01-29 17:32:16,591 xworld_map.py:45] finish loading ../empty_ground.json
[INFO 2018-01-29 17:32:17,420 main_a3c.py:58] Episode 0	average cumulative reward: -8.45
[INFO 2018-01-29 17:32:17,420 main_a3c.py:63] Episode 0	saving model: 00000.pth
Process SpawnProcess-3:
Traceback (most recent call last):
  File "/usr/lib/python3.5/multiprocessing/process.py", line 249, in _bootstrap
    self.run()
  File "/usr/lib/python3.5/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/yi/code/xworld_learning/experiment/push_box_target/exp001-new/main_a3c.py", line 66, in train
    solver.optimize()
  File "/home/yi/code/xworld_learning/learning/async_actor_critic.py", line 65, in optimize
    action.reinforce(r - value.data[0,0])
  File "/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py", line 232, in reinforce
    """))
RuntimeError: reinforce() was removed.
Use torch.distributions instead.
See http://pytorch.org/docs/master/distributions.html

Instead of:

probs = policy_network(state)
action = probs.multinomial()
next_state, reward = env.step(action)
action.reinforce(reward)
action.backward()

Use:

probs = policy_network(state)
# NOTE: categorical is equivalent to what used to be called multinomial
m = torch.distributions.Categorical(probs)
action = m.sample()
next_state, reward = env.step(action)
loss = -m.log_prob(action) * reward
loss.backward()

[INFO 2018-01-29 17:32:17,480 main_a3c.py:58] Episode 0	average cumulative reward: -10.33
[INFO 2018-01-29 17:32:17,480 main_a3c.py:63] Episode 0	saving model: 00000.pth
Process SpawnProcess-2:
Traceback (most recent call last):
  File "/usr/lib/python3.5/multiprocessing/process.py", line 249, in _bootstrap
    self.run()
  File "/usr/lib/python3.5/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/yi/code/xworld_learning/experiment/push_box_target/exp001-new/main_a3c.py", line 66, in train
    solver.optimize()
  File "/home/yi/code/xworld_learning/learning/async_actor_critic.py", line 65, in optimize
    action.reinforce(r - value.data[0,0])
  File "/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py", line 232, in reinforce
    """))
RuntimeError: reinforce() was removed.
Use torch.distributions instead.
See http://pytorch.org/docs/master/distributions.html

Instead of:

probs = policy_network(state)
action = probs.multinomial()
next_state, reward = env.step(action)
action.reinforce(reward)
action.backward()

Use:

probs = policy_network(state)
# NOTE: categorical is equivalent to what used to be called multinomial
m = torch.distributions.Categorical(probs)
action = m.sample()
next_state, reward = env.step(action)
loss = -m.log_prob(action) * reward
loss.backward()

[INFO 2018-01-29 17:32:17,526 main_a3c.py:58] Episode 0	average cumulative reward: -11.52
[INFO 2018-01-29 17:32:17,526 main_a3c.py:63] Episode 0	saving model: 00000.pth
Process SpawnProcess-4:
Traceback (most recent call last):
  File "/usr/lib/python3.5/multiprocessing/process.py", line 249, in _bootstrap
    self.run()
  File "/usr/lib/python3.5/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/yi/code/xworld_learning/experiment/push_box_target/exp001-new/main_a3c.py", line 66, in train
    solver.optimize()
  File "/home/yi/code/xworld_learning/learning/async_actor_critic.py", line 65, in optimize
    action.reinforce(r - value.data[0,0])
  File "/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py", line 232, in reinforce
    """))
RuntimeError: reinforce() was removed.
Use torch.distributions instead.
See http://pytorch.org/docs/master/distributions.html

Instead of:

probs = policy_network(state)
action = probs.multinomial()
next_state, reward = env.step(action)
action.reinforce(reward)
action.backward()

Use:

probs = policy_network(state)
# NOTE: categorical is equivalent to what used to be called multinomial
m = torch.distributions.Categorical(probs)
action = m.sample()
next_state, reward = env.step(action)
loss = -m.log_prob(action) * reward
loss.backward()

[INFO 2018-01-29 17:32:17,937 main_a3c.py:58] Episode 0	average cumulative reward: -8.48
[INFO 2018-01-29 17:32:17,937 main_a3c.py:63] Episode 0	saving model: 00000.pth
Process SpawnProcess-1:
Traceback (most recent call last):
  File "/usr/lib/python3.5/multiprocessing/process.py", line 249, in _bootstrap
    self.run()
  File "/usr/lib/python3.5/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/yi/code/xworld_learning/experiment/push_box_target/exp001-new/main_a3c.py", line 66, in train
    solver.optimize()
  File "/home/yi/code/xworld_learning/learning/async_actor_critic.py", line 65, in optimize
    action.reinforce(r - value.data[0,0])
  File "/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py", line 232, in reinforce
    """))
RuntimeError: reinforce() was removed.
Use torch.distributions instead.
See http://pytorch.org/docs/master/distributions.html

Instead of:

probs = policy_network(state)
action = probs.multinomial()
next_state, reward = env.step(action)
action.reinforce(reward)
action.backward()

Use:

probs = policy_network(state)
# NOTE: categorical is equivalent to what used to be called multinomial
m = torch.distributions.Categorical(probs)
action = m.sample()
next_state, reward = env.step(action)
loss = -m.log_prob(action) * reward
loss.backward()

